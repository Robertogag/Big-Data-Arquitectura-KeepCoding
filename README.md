# Pr谩ctica Big Data Arquitectura

## Objetivos del Proyecto
 Dado el gran n煤mero de coleccionistas y entusiastas en todo el mundo, la plataforma de an谩lisis de cartas coleccionables Pok茅mon y Yu-Gi-Oh tiene un amplio p煤blico objetivo.

 Ofrecer datos en tiempo real, an谩lisis precisos y proyecciones de precios como diferenciador clave.

 Potencial de expansi贸n a otros juegos de cartas coleccionables.

 Transformar el acceso a la informaci贸n en una fuente de ingresos a trav茅s de suscripciones.

## Modelo de Negocio
 Tarifas de suscripci贸n recurrentes mensuales o anuales con diferentes niveles de acceso.

 Exploraci贸n de oportunidades de publicidad dirigida y colaboraciones con socios del mercado de cartas coleccionables.

## Oportunidades de Crecimiento
 **Ampliaci贸n de Contenido:** A帽adir m谩s juegos de cartas coleccionables a la plataforma.

 **Expansi贸n Global:** Llegar a audiencias internacionales y localizadas.

 **Integraci贸n de API:** Ampliar servicios a trav茅s de integraciones con otras plataformas y servicios.

 **Desarrollo de Aplicaciones M贸viles:** Lanzar aplicaciones m贸viles para un acceso m谩s conveniente.

## Definici贸n DAaaS
La estrategia para el Data Analytics as a Service (DAaaS) se centra en proporcionar una plataforma robusta y completa para el an谩lisis de cartas coleccionables. El cat谩logo de servicios incluye:

1.  **Incorporaci贸n de Datos:** Recopilaci贸n autom谩tica de informaci贸n de cartas, precios y comentarios de usuarios desde fuentes en l铆nea, utilizando web scrapers y crawlers programados.
2. Ч **Limpieza de Datos:** Procesamiento y transformaci贸n de datos para garantizar la calidad y precisi贸n de la informaci贸n recopilada.
3.  **Transformaci贸n de Datos:** Preparaci贸n de datos para su an谩lisis, incluyendo la estructuraci贸n de la informaci贸n y la creaci贸n de conjuntos de datos coherentes.
4.  **Datapedias:** Creaci贸n de repositorios de datos y metadatos para facilitar la comprensi贸n de los datos recopilados.
5. О **Bibliotecas de Herramientas Anal铆ticas:** Proporcionar acceso a herramientas de an谩lisis de datos, visualizaci贸n y generaci贸n de proyecciones de precios.

## Arquitectura DAaaS
La arquitectura del DAaaS se basa en componentes clave:

1.  **P谩gina Web Interactiva:** Una interfaz de usuario intuitiva que permite a los usuarios explorar cartas, ver precios y estad铆sticas, y acceder a informaci贸n valiosa.
2. 锔 **Google Cloud Platform (GCP):** La plataforma se aloja en GCP, lo que garantiza escalabilidad, seguridad y confiabilidad.
3.  **Datos en Tiempo Real:** Ofrecer datos actualizados al instante, incluyendo an谩lisis de precios, comentarios de usuarios y proyecciones.
4.  **Fuentes de Datos Confiables:** Utilizar web crawlers y algoritmos de recopilaci贸n de datos para mantener la informaci贸n precisa y actualizada.

## DAaaS Operating Model Design
### Recopilaci贸n de Datos y Actualizaci贸n
#### Web Scraper y Crawler:
-  Implementa web scrapers y crawlers programados con Python.
-  Extraen informaci贸n desde fuentes en l铆nea como sitios web de subastas y foros de discusi贸n.
-  Programados para ejecutarse autom谩ticamente cada 12 horas utilizando Cloud Functions y un Scheduler.

### Almacenamiento de Datos y Procesamiento
#### Google Cloud Storage:
-  Los datos recopilados por los crawlers se almacenan en Google Cloud Storage, actuando como una ubicaci贸n centralizada y escalable.

### An谩lisis de Sentimientos y Comentarios
#### Web Scraper para Comentarios:
-  Se utiliza un web scraper adicional para recopilar comentarios de usuarios desde foros y redes sociales.
#### Elasticsearch:
-  Los comentarios y opiniones de usuarios se almacenan en Elasticsearch.

### Interacci贸n con Usuarios y Actualizaci贸n en Tiempo Real
#### P谩gina Web:
-  La p谩gina web interact煤a con BigQuery, Google Cloud Storage para proporcionar a los usuarios acceso a datos, an谩lisis y proyecciones en tiempo real.
#### Kafka:
-  Kafka se encarga de transmitir datos en tiempo real entre los diferentes componentes de la arquitectura, asegurando informaci贸n actualizada al instante.

### Programaci贸n y Automatizaci贸n
#### Scheduler:
-  Se programa un Scheduler para ejecutar tareas peri贸dicas, como la actualizaci贸n de datos de los crawlers desde fuentes en l铆nea cada 12 horas y el an谩lisis de sentimientos en los comentarios de usuarios diariamente.




## Diagrama

![image](https://github.com/Robertogag/Big-Data-Arquitectura-KeepCoding/assets/137840110/ba6c1321-432d-48bf-84c4-c0bdc3cb0d37)




